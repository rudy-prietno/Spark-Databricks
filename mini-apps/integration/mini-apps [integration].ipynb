{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac4ccd8c-1c72-46ee-a393-0ba2460d5191",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Preparation Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2de3183f-9940-46ca-b3f2-7e10a3f5a358",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Move the file to the correct path\n",
    "dbutils.fs.cp(\"dbfs:/FileStore/tables/coreFunction.py\", \"file:/databricks/driver/coreFunction.py\")\n",
    "\n",
    "# Add the directory to the system path\n",
    "import sys\n",
    "sys.path.append(\"/databricks/driver/\")\n",
    "\n",
    "# Now, import the coreFunction module\n",
    "import coreFunction\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DateType, DecimalType, TimestampType\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9af11fd-495a-4190-b9d3-b55ac86ff561",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Extract from File.xlsx Load to Bronze (Delta Table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c7d2c81-2d0a-4fab-8bb0-7e63992df42b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- AccountNo: string (nullable = true)\n |-- Date: date (nullable = true)\n |-- TransactionDetails: string (nullable = true)\n |-- CHQNo: string (nullable = true)\n |-- ValueDate: date (nullable = true)\n |-- WithdrawalAMT: decimal(18,2) (nullable = true)\n |-- DepositAMT: decimal(18,2) (nullable = true)\n |-- BalanceAMT: decimal(18,2) (nullable = true)\n |-- IngestionTime: timestamp (nullable = false)\n\nError: [DELTA_TABLE_NOT_FOUND] Delta table `bronze_data`.`transactions_raw` doesn't exist.\nbronze_data.transactions_raw is not a Delta table, creating a new Delta table.\n116201 rows written to bronze_data.transactions_raw.\n"
     ]
    }
   ],
   "source": [
    "# Copy the file from DBFS to the local file system\n",
    "dbutils.fs.cp(\n",
    "    \"dbfs:/FileStore/tables/Example.xlsx\", \n",
    "    \"file:/tmp/Example.xlsx\"\n",
    "    )\n",
    "\n",
    "file_path = \"/tmp/Example.xlsx\"\n",
    "sheet_name = \"Sheet1\" \n",
    "\n",
    "# Instantiate the DataReader class\n",
    "readerSource = coreFunction.DataReader(file_path, sheet_name)\n",
    "dataSource = readerSource.readExcell()\n",
    "\n",
    "# Rename column to support naming regulation on delta pattern\n",
    "dataSource = dataSource.rename(columns={\n",
    "    'Account No': 'AccountNo',\n",
    "    'DATE':'Date',\n",
    "    'TRANSACTION DETAILS': 'TransactionDetails',\n",
    "    'CHQ.NO.': 'CHQNo',\n",
    "    'VALUE DATE': 'ValueDate',\n",
    "    'WITHDRAWAL AMT':'WithdrawalAMT',\n",
    "    'DEPOSIT AMT':'DepositAMT', \t\n",
    "    'BALANCE AMT':'BalanceAMT'\n",
    "})\n",
    "\n",
    "# Remove the extra apostrophe (') from the 'Account No' column\n",
    "dataSource['AccountNo'] = dataSource['AccountNo'].str.replace(\"'\", \"\")\n",
    "\n",
    "# change type data\n",
    "dataSource['Date'] = pd.to_datetime(dataSource['Date'], errors='coerce')\n",
    "dataSource['ValueDate'] = pd.to_datetime(dataSource['ValueDate'], errors='coerce') \n",
    "dataSource['TransactionDetails'] = dataSource['TransactionDetails'].astype(str)\n",
    "dataSource['AccountNo'] = dataSource['AccountNo'].astype(str)\n",
    "dataSource['CHQNo'] = dataSource['CHQNo'].astype(str)\n",
    "\n",
    "dataSource['WithdrawalAMT'] = dataSource['WithdrawalAMT'].apply(coreFunction.convert_to_decimal)\n",
    "dataSource['DepositAMT'] = dataSource['DepositAMT'].apply(coreFunction.convert_to_decimal)\n",
    "dataSource['BalanceAMT'] = dataSource['BalanceAMT'].apply(coreFunction.convert_to_decimal)\n",
    "\n",
    "# Drop the column with the name '.'\n",
    "dataSource_cleaned = dataSource.drop(columns=['.'])\n",
    "\n",
    "\n",
    "# Define structure schema for spark DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"AccountNo\", StringType(), True),\n",
    "    StructField(\"Date\", DateType(), True),\n",
    "    StructField(\"TransactionDetails\", StringType(), True),\n",
    "    StructField(\"CHQNo\", StringType(), True),\n",
    "    StructField(\"ValueDate\", DateType(), True),\n",
    "    StructField(\"WithdrawalAMT\", DecimalType(18, 2), True),\n",
    "    StructField(\"DepositAMT\", DecimalType(18, 2), True),\n",
    "    StructField(\"BalanceAMT\", DecimalType(18, 2), True)\n",
    "])\n",
    "\n",
    "# convert from pandas dataframe to spark\n",
    "spark_df = spark.createDataFrame(dataSource_cleaned, schema=schema)\n",
    "\n",
    "# convert UTC to Asia/Jakarta\n",
    "spark_df = spark_df.withColumn(\"IngestionTime\", F.current_timestamp().cast(TimestampType()))\n",
    "spark_df = spark_df.withColumn(\"IngestionTime\", F.from_utc_timestamp(F.col(\"IngestionTime\"), \"Asia/Jakarta\"))\n",
    "\n",
    "# show structure schema\n",
    "spark_df.printSchema()\n",
    "\n",
    "tableName = \"bronze_data.transactions_raw\"\n",
    "coreFunction.DataIngestion.DeltaTables(tableName, spark_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc2c406a-59ae-4dd8-aa38-09b7886cb100",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Data Quality: File.xlsx vs Bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cd2625a-2805-4190-9104-6d8dfcf03f84",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4412b310a4c74aaf924f64368aed2dc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4c11e639b3d4c54858e3dc5aeaa8be3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 'quality_data.quality_monitoring' is an existing Delta table.\n[INFO] Created DataFrame for DQ results: {'ID': '015fec2701f45377de874e82e46009a5b45e595089812f62aa31f1c87f5c6ad5', 'Updated_Date': '2024-09-11', 'Updated_Timestamp': '2024-09-11 19:21:40', 'Type': 'integration', 'Process': 'xlsx_to_bronze', 'Type_File_Sources': 'xlsx', 'File_Sources': '/tmp/Example.xlsx', 'Table_Name': 'transactions_raw', 'Status_Observation': 'PASS', 'Source_Observation': 116201, 'Destination_Observation': 116201, 'Difference_Observation': 0, 'Status_Rows': 'PASS', 'Source_Rows': 10, 'Destination_Rows': 10, 'Difference_Rows': 0}.\n[INFO] Successfully merged DQ results into the Delta table.\nNumber of rows updated: 1\nNumber of rows inserted: 0\n"
     ]
    }
   ],
   "source": [
    "# Sources\n",
    "profiler_xlsx = coreFunction.DataProfiling(titleProfile=\"Profiling Report XLSX\")\n",
    "\n",
    "\n",
    "# if the time is not provided, you can fill it with '00:00:00'\n",
    "spark_dfProfile = spark_df.withColumn(\"Date\", F.coalesce(F.col(\"Date\"), F.lit(\"00:00:00\").cast(TimestampType())))\n",
    "spark_dfProfile = spark_dfProfile.withColumn(\"ValueDate\", F.coalesce(F.col(\"ValueDate\"), F.lit(\"00:00:00\").cast(TimestampType())))\n",
    "\n",
    "# Convert DecimalType columns to float for profiling or operations that do not support DecimalType\n",
    "spark_dfProfile = spark_dfProfile.withColumn(\"WithdrawalAMT\", F.col(\"WithdrawalAMT\").cast(\"float\"))\n",
    "spark_dfProfile = spark_dfProfile.withColumn(\"DepositAMT\", F.col(\"DepositAMT\").cast(\"float\"))\n",
    "spark_dfProfile = spark_dfProfile.withColumn(\"BalanceAMT\", F.col(\"BalanceAMT\").cast(\"float\"))\n",
    "\n",
    "# Now convert to pandas DataFrame for profiling\n",
    "pandas_df = spark_dfProfile.toPandas()\n",
    "\n",
    "profile_source =profiler_xlsx.profile(pandas_df)\n",
    "\n",
    "source_observation= profile_source.description_set.table['n']\n",
    "source_rows= profile_source.description_set.variables['AccountNo']['n_distinct']\n",
    "\n",
    "# Destination\n",
    "bronze_df = spark.read.format(\"delta\").table(\"bronze_data.transactions_raw\").select(\"AccountNo\")\n",
    "bronze_df_profile = bronze_df.toPandas()\n",
    "\n",
    "profiler_bronze = coreFunction.DataProfiling(titleProfile=\"Profiling Report Bronze\")\n",
    "profiler_destination =profiler_bronze.profile(bronze_df_profile)\n",
    "\n",
    "destination_observation= profiler_destination.description_set.table['n']\n",
    "destination_rows= profiler_destination.description_set.variables['AccountNo']['n_distinct']\n",
    "\n",
    "coreFunction.dataQuality.generate_data_quality_report(\n",
    "            tableDQ= 'quality_data.quality_monitoring',\n",
    "            source_observation= source_observation, \n",
    "            destination_observation= destination_observation, \n",
    "            source_rows= source_rows, \n",
    "            destination_rows= destination_rows, \n",
    "            table_name= 'transactions_raw',\n",
    "            process= 'xlsx_to_bronze',\n",
    "            etlType= 'integration',\n",
    "            dataFormat= 'xlsx',\n",
    "            file_path= '/tmp/Example.xlsx'\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "mini-apps [integration]",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
