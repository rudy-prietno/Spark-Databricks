{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "514866fb-7c91-4a6b-9a85-4918f7be0f0a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Preparation Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a1d3142-c84b-425d-81ed-adef5429d031",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Move the file to the correct path\n",
    "dbutils.fs.cp(\"dbfs:/FileStore/tables/coreFunction.py\", \"file:/databricks/driver/coreFunction.py\")\n",
    "\n",
    "# Add the directory to the system path\n",
    "import sys\n",
    "sys.path.append(\"/databricks/driver/\")\n",
    "\n",
    "# Now, import the coreFunction module\n",
    "import coreFunction\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54bb7dc3-4c6f-4865-8eba-a1a3062388ac",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Extract from Bronze - Transform in Dataframe - Load to Silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ce322cf-7589-40b8-baa1-1639c3688ec0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta table not found or error occurred: [DELTA_TABLE_NOT_FOUND] Delta table `silver_data`.`transactions_clean` doesn't exist.\nCreating new Delta table and writing data...\n116156 rows written to silver_data.transactions_clean\n"
     ]
    }
   ],
   "source": [
    "# Generated Unique ID for make a Primary Key\n",
    "# Load the bronze Delta table into a DataFrame\n",
    "df_bronze = spark.read.format(\"delta\").table(\"bronze_data.transactions_raw\")\n",
    "\n",
    "# Adjust the unique ID generation by considering more columns for uniqueness\n",
    "df_bronze_preparation = df_bronze.withColumn(\n",
    "    \"Id\", \n",
    "    F.sha2(F.concat(\n",
    "        F.col(\"AccountNo\"),\n",
    "        F.col(\"Date\"),\n",
    "        F.col(\"TransactionDetails\"),\n",
    "        F.coalesce(F.col(\"CHQNo\").cast(\"string\"), F.lit('')),\n",
    "        F.coalesce(F.col(\"WithdrawalAMT\").cast(\"string\"), F.lit('')),\n",
    "        F.coalesce(F.col(\"DepositAMT\").cast(\"string\"), F.lit('')),\n",
    "        F.coalesce(F.col(\"BalanceAMT\").cast(\"string\"), F.lit(''))\n",
    "    ), 256)\n",
    ")\n",
    "\n",
    "\n",
    "# SQL query to select data from your DataFrame\n",
    "df_bronze_preparation.createOrReplaceTempView(\"silver_data_table\")\n",
    "\n",
    "# Define partition and order keys to make sure data is unique\n",
    "partitionKeys = ['Id']\n",
    "orderKeys = ['Date', 'IngestionTime']\n",
    "\n",
    "# Define primary and secondary partition columns to build a partition delta table\n",
    "partitionColumns = 'Date'\n",
    "partitionColumnSecondary = 'ValueDate'\n",
    "\n",
    "queries = \"\"\"\n",
    "SELECT * FROM silver_data_table\n",
    "\"\"\"\n",
    "\n",
    "# Use the DeltaTablesPartition function, enabling schema merge\n",
    "coreFunction.DataIngestion.DeltaTablesPartition(\n",
    "    queries=queries,\n",
    "    tableName=\"silver_data.transactions_clean\",  # Delta table name\n",
    "    partitionKeys=partitionKeys,\n",
    "    orderKeys=orderKeys,\n",
    "    partitionColumns=partitionColumns,\n",
    "    partitionColumnSecondary=partitionColumnSecondary\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6e0c0fa-12c2-4296-a2e8-48e61c092dc3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Data Quality: Bronze vs Silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53879fb8-ff80-4ec7-a147-10186222259b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c98a0188603f4c1893f379a420379715",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fcec600a4704ac5a05aa76ab4c5aa40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 'quality_data.quality_monitoring' is an existing Delta table.\n[INFO] Created DataFrame for DQ results: {'ID': '1badaa2ba4c1f4cf39b44ba35493d5d9c0f163e8508d0a6b7da31e2898ae7023', 'Updated_Date': '2024-09-11', 'Updated_Timestamp': '2024-09-11 19:25:46', 'Type': 'platform', 'Process': 'bronze_to_silver', 'Type_File_Sources': 'delta', 'File_Sources': 'dbfs:/user/hive/warehouse/bronze_data.db/transactions_raw', 'Table_Name': 'transactions_clean', 'Status_Observation': 'FAIL', 'Source_Observation': 116201, 'Destination_Observation': 116156, 'Difference_Observation': 45, 'Status_Rows': 'PASS', 'Source_Rows': 116156, 'Destination_Rows': 116156, 'Difference_Rows': 0}.\n[INFO] Successfully merged DQ results into the Delta table.\nNumber of rows updated: 1\nNumber of rows inserted: 0\n"
     ]
    }
   ],
   "source": [
    "# Source\n",
    "bronze_df_source = spark.read.format(\"delta\").table(\"bronze_data.transactions_raw\")\n",
    "\n",
    "df_bronze_preparation = bronze_df_source.withColumn(\n",
    "    \"Id\", \n",
    "    F.sha2(F.concat(\n",
    "        F.col(\"AccountNo\"),\n",
    "        F.col(\"Date\"),\n",
    "        F.col(\"TransactionDetails\"),\n",
    "        F.coalesce(F.col(\"CHQNo\").cast(\"string\"), F.lit('')),\n",
    "        F.coalesce(F.col(\"WithdrawalAMT\").cast(\"string\"), F.lit('')),\n",
    "        F.coalesce(F.col(\"DepositAMT\").cast(\"string\"), F.lit('')),\n",
    "        F.coalesce(F.col(\"BalanceAMT\").cast(\"string\"), F.lit(''))\n",
    "    ), 256)\n",
    ")\n",
    "\n",
    "bronze_df = df_bronze_preparation.select(\"Id\")\n",
    "\n",
    "bronze_df_profile = bronze_df.toPandas()\n",
    "\n",
    "profiler_bronze = coreFunction.DataProfiling(titleProfile=\"Profiling Report Bronze\")\n",
    "profiler_sources =profiler_bronze.profile(bronze_df_profile)\n",
    "\n",
    "source_observation= profiler_sources.description_set.table['n']\n",
    "source_rows= profiler_sources.description_set.variables['Id']['n_distinct']\n",
    "\n",
    "\n",
    "# Destination\n",
    "silver_df = spark.read.format(\"delta\").table(\"silver_data.transactions_clean\").select(\"Id\")\n",
    "silver_df_profile = silver_df.toPandas()\n",
    "\n",
    "profiler_silver = coreFunction.DataProfiling(titleProfile=\"Profiling Report Silver\")\n",
    "profiler_destination =profiler_silver.profile(silver_df_profile)\n",
    "\n",
    "destination_observation= profiler_destination.description_set.table['n']\n",
    "destination_rows= profiler_destination.description_set.variables['Id']['n_distinct']\n",
    "\n",
    "coreFunction.dataQuality.generate_data_quality_report(\n",
    "            tableDQ= 'quality_data.quality_monitoring',\n",
    "            source_observation= source_observation, \n",
    "            destination_observation= destination_observation, \n",
    "            source_rows= source_rows, \n",
    "            destination_rows= destination_rows, \n",
    "            table_name= 'transactions_clean',\n",
    "            process= 'bronze_to_silver',\n",
    "            etlType= 'platform',\n",
    "            dataFormat= 'delta',\n",
    "            file_path= 'dbfs:/user/hive/warehouse/bronze_data.db/transactions_raw'\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "mini-apps [platform]",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
